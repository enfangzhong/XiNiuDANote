{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark机器学习基础\n",
    "by [寒小阳](http://blog.csdn.net/han_xiaoyang) (hanxiaoyang.ml@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 0.K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1:0.0 2:0.0 3:0.0\r\n",
      "1 1:0.1 2:0.1 3:0.1\r\n",
      "2 1:0.2 2:0.2 3:0.2\r\n",
      "3 1:9.0 2:9.0 3:9.0\r\n",
      "4 1:9.1 2:9.1 3:9.1\r\n"
     ]
    }
   ],
   "source": [
    "! head -5 data/mllib/sample_kmeans_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "predicted Center: \n",
      "{'prediction': 0}\n",
      "{'prediction': 0}\n",
      "{'prediction': 0}\n",
      "{'prediction': 1}\n",
      "{'prediction': 1}\n",
      "{'prediction': 1}\n",
      "Cluster Centers: \n",
      "[ 0.1  0.1  0.1]\n",
      "[ 9.1  9.1  9.1]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"KMeansExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# 训练K-means聚类模型\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# 预测(即分配聚类中心)\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# 根据Silhouette得分评估(pyspark2.2里新加)\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# 输出预测结果\n",
    "print(\"predicted Center: \")\n",
    "for center in predictions[['prediction']].collect():\n",
    "    print(center.asDict())\n",
    "\n",
    "# 聚类中心\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.GMM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussians shown as a DataFrame: \n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|mean                                                         |cov                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[9.099999999999984,9.099999999999984,9.099999999999984]      |0.006666666666812185  0.006666666666812185  0.006666666666812185  \n",
      "0.006666666666812185  0.006666666666812185  0.006666666666812185  \n",
      "0.006666666666812185  0.006666666666812185  0.006666666666812185  |\n",
      "|[0.10000000000001552,0.10000000000001552,0.10000000000001552]|0.006666666666806454  0.006666666666806454  0.006666666666806454  \n",
      "0.006666666666806454  0.006666666666806454  0.006666666666806454  \n",
      "0.006666666666806454  0.006666666666806454  0.006666666666806454  |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"GaussianMixtureExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(0)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "model.gaussiansDF.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.关联规则\n",
    "我这里是pyspark 2.2以下的版本的写法，新版可以参考此程序之下的程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r z h k p\r\n",
      "z y x w v u t s\r\n",
      "s x o n r\r\n",
      "x z y m t s q e\r\n",
      "z\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/mllib/sample_fpgrowth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['z'], freq=5)\n",
      "FreqItemset(items=['x'], freq=4)\n",
      "FreqItemset(items=['x', 'z'], freq=3)\n",
      "FreqItemset(items=['y'], freq=3)\n",
      "FreqItemset(items=['y', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 'z'], freq=3)\n",
      "FreqItemset(items=['r'], freq=3)\n",
      "FreqItemset(items=['r', 'x'], freq=2)\n",
      "FreqItemset(items=['r', 'z'], freq=2)\n",
      "FreqItemset(items=['s'], freq=3)\n",
      "FreqItemset(items=['s', 'y'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'x'], freq=3)\n",
      "FreqItemset(items=['s', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'z'], freq=2)\n",
      "FreqItemset(items=['t'], freq=3)\n",
      "FreqItemset(items=['t', 'y'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 's'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'z'], freq=3)\n",
      "FreqItemset(items=['p'], freq=2)\n",
      "FreqItemset(items=['p', 'r'], freq=2)\n",
      "FreqItemset(items=['p', 'r', 'z'], freq=2)\n",
      "FreqItemset(items=['p', 'z'], freq=2)\n",
      "FreqItemset(items=['q'], freq=2)\n",
      "FreqItemset(items=['q', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'z'], freq=2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"FPGrowthExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = spark.sparkContext.textFile(\"data/mllib/sample_fpgrowth.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|      [1]|   3|\n",
      "|      [2]|   3|\n",
      "|   [2, 1]|   3|\n",
      "|      [5]|   2|\n",
      "|   [5, 2]|   2|\n",
      "|[5, 2, 1]|   2|\n",
      "|   [5, 1]|   2|\n",
      "+---------+----+\n",
      "\n",
      "+----------+----------+------------------+\n",
      "|antecedent|consequent|        confidence|\n",
      "+----------+----------+------------------+\n",
      "|    [5, 2]|       [1]|               1.0|\n",
      "|       [2]|       [1]|               1.0|\n",
      "|       [2]|       [5]|0.6666666666666666|\n",
      "|    [2, 1]|       [5]|0.6666666666666666|\n",
      "|       [5]|       [2]|               1.0|\n",
      "|       [5]|       [1]|               1.0|\n",
      "|    [5, 1]|       [2]|               1.0|\n",
      "|       [1]|       [2]|               1.0|\n",
      "|       [1]|       [5]|0.6666666666666666|\n",
      "+----------+----------+------------------+\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|       items|prediction|\n",
      "+---+------------+----------+\n",
      "|  0|   [1, 2, 5]|        []|\n",
      "|  1|[1, 2, 3, 5]|        []|\n",
      "|  2|      [1, 2]|       [5]|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"FPGrowthExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], [\"id\", \"items\"])\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
    "model = fpGrowth.fit(df)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model.transform(df).show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.LDA主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1:1 2:2 3:6 4:0 5:2 6:3 7:1 8:1 9:0 10:0 11:3\r\n",
      "1 1:1 2:3 3:0 4:1 5:3 6:0 7:0 8:2 9:0 10:0 11:1\r\n",
      "2 1:1 2:4 3:1 4:0 5:0 6:4 7:9 8:0 9:1 10:2 11:0\r\n",
      "3 1:2 2:1 3:0 4:3 5:0 6:0 7:5 8:0 9:2 10:3 11:9\r\n",
      "4 1:3 2:1 3:1 4:9 5:3 6:0 7:2 8:0 9:0 10:1 11:3\r\n"
     ]
    }
   ],
   "source": [
    "! head -5 data/mllib/sample_lda_libsvm_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -784.563312043982\n",
      "The upper bound on perplexity: 3.0175511876689556\n",
      "\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[3, 4, 10] |[0.30479149989901694, 0.12278796514250867, 0.12161706973908994]|\n",
      "|1    |[10, 5, 1] |[0.10472118214638892, 0.10407538205701086, 0.09443251272060076]|\n",
      "|2    |[0, 9, 8]  |[0.10725946461445685, 0.10401349116870545, 0.09445471149622425]|\n",
      "|3    |[5, 0, 4]  |[0.1861359631937542, 0.15982888484836216, 0.15787541409980693] |\n",
      "|4    |[6, 1, 9]  |[0.22100346160439127, 0.1886216316667507, 0.13487077710226394] |\n",
      "|5    |[0, 4, 8]  |[0.11475105421219843, 0.0938293926323442, 0.09373655278318598] |\n",
      "|6    |[10, 0, 1] |[0.09936287293318637, 0.09919070503430866, 0.09851897075744546]|\n",
      "|7    |[1, 9, 2]  |[0.1014842342649455, 0.10140920563411741, 0.09915451366273027] |\n",
      "|8    |[3, 1, 10] |[0.1106449222406801, 0.0947496161356481, 0.0936939041532816]   |\n",
      "|9    |[9, 8, 2]  |[0.10413531876664031, 0.09732819093421781, 0.09628569499072241]|\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "transform dataset:\n",
      "\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                     |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.005060343096754452,0.004779224371126995,0.004779167378819419,0.15513698910982235,0.8063483198718517,0.004779154342319572,0.004779234485359031,0.004779217257466835,0.004779171739715741,0.0047791783467639225]     |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.2734569771626308,0.007973271153189363,0.007973145528683334,0.6624454938194773,0.008284854792904294,0.007973165379813392,0.007973326313885832,0.0079732955271774,0.00797328963040972,0.007973180691828369]          |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004398208855890409,0.004154555823485665,0.004154568313800778,0.004396437855991017,0.9621234556415211,0.004154559175307323,0.004154551127869646,0.004154545212651145,0.00415455229580871,0.004154565697674274]      |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.6787941120549785,0.003674797670337556,0.003674809800743196,0.00388822265153777,0.29159411433883414,0.003674763199203662,0.0036748002044778632,0.003674794382671649,0.003674796622303754,0.003674789074911846]      |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.9637819545598443,0.003981248640615379,0.0039812613448832425,0.004212739273390889,0.004136452680163433,0.0039812630784879025,0.00398127114213346,0.003981273511943595,0.003981275364463161,0.003981260404074483]    |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.19173102233829598,0.003674781484271388,0.003674801519586637,0.7787270860519163,0.0038183220217994297,0.003674780368884357,0.003674793606388757,0.0036747881150433846,0.003674810825261795,0.0036748136685521367]   |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.7226473135754939,0.0038219773493048757,0.0038219878058903646,0.004043939336427409,0.24655493897624053,0.00382194611566146,0.003821979061674455,0.0038219728641123306,0.0038219781605772257,0.003821966754617474]   |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.9604830307208936,0.004343842533119806,0.004343793372195523,0.004596876945426264,0.004513421993024098,0.004343786032703549,0.004343817897556423,0.004343778324969421,0.004343844650990688,0.004343807529120705]     |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.24864548138939338,0.004343817474845246,0.004343792252522897,0.7164342819986811,0.004513496682561095,0.0043438192976202284,0.004343839843285602,0.004343822376987891,0.004343844731122574,0.0043438039529800085]    |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.0034880595878568036,0.003294211880615744,0.003294229780385521,0.003485959478940128,0.9699663855889723,0.0032942157751267532,0.0032942419492447837,0.0032942492075424155,0.0032942141953274653,0.003294232555987996]|\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.7988488921883795,0.004154742410116491,0.004154706134109581,0.004396690060548525,0.16767145607288966,0.0041546708289307895,0.004154713475866729,0.004154671910228219,0.004154740320060146,0.004154716598870316]     |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.0050591793404772,0.004778739606097657,0.004778746459172331,0.956524537983852,0.004965109153328436,0.004778750208331833,0.0047787466313080505,0.004778726294753947,0.00477872908058444,0.004778735242093942]        |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"LDAExample\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 加载数据\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_lda_libsvm_data.txt\")\n",
    "\n",
    "# 训练LDA模型\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp)+\"\\n\")\n",
    "\n",
    "# 输出主题\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# 数据集解析\n",
    "print(\"transform dataset:\\n\")\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PCAExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 构建一份fake data\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# PCA降维\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      word|              vector|\n",
      "+----------+--------------------+\n",
      "|     heard|[-0.1366093307733...|\n",
      "|       are|[-0.1653077900409...|\n",
      "|      neat|[0.09378280490636...|\n",
      "|   classes|[-0.1547942310571...|\n",
      "|         I|[-0.0424778945744...|\n",
      "|regression|[-0.0461251139640...|\n",
      "|  Logistic|[0.02324013225734...|\n",
      "|     Spark|[-0.0981360152363...|\n",
      "|     could|[-0.0302416980266...|\n",
      "|       use|[0.02537945471704...|\n",
      "|        Hi|[-0.0277608968317...|\n",
      "|    models|[-0.1365544795989...|\n",
      "|      case|[-0.1623256206512...|\n",
      "|     about|[0.03644379600882...|\n",
      "|      Java|[-0.1164701506495...|\n",
      "|      wish|[-0.0754781961441...|\n",
      "+----------+--------------------+\n",
      "\n",
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.0537080682814,0.0383701570332,0.0819620683789]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.079486905198,0.0158437477159,0.0428948812187]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.0461928892881,0.0157909940928,-0.0516870014369]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Word2VecExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 输入是bag of words形式\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# 设置窗口长度等参数，词嵌入学习\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "# 输出词和词向量\n",
    "model.getVectors().show()\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
