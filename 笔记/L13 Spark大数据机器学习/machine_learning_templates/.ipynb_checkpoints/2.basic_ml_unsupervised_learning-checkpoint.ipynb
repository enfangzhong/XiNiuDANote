{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark机器学习基础\n",
    "by [寒小阳](http://blog.csdn.net/han_xiaoyang) (hanxiaoyang.ml@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 0.K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.clustering import KMeans\n",
    "#from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1:0.0 2:0.0 3:0.0\r\n",
      "1 1:0.1 2:0.1 3:0.1\r\n",
      "2 1:0.2 2:0.2 3:0.2\r\n",
      "3 1:9.0 2:9.0 3:9.0\r\n",
      "4 1:9.1 2:9.1 3:9.1\r\n"
     ]
    }
   ],
   "source": [
    "! head -5 data/mllib/sample_kmeans_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Center: \n",
      "{'prediction': 0}\n",
      "{'prediction': 0}\n",
      "{'prediction': 0}\n",
      "{'prediction': 1}\n",
      "{'prediction': 1}\n",
      "{'prediction': 1}\n",
      "Cluster Centers: \n",
      "[ 0.1  0.1  0.1]\n",
      "[ 9.1  9.1  9.1]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"KMeansExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# 训练K-means聚类模型\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# 预测(即分配聚类中心)\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# 根据Silhouette得分评估(pyspark2.2里新加)\n",
    "#evaluator = ClusteringEvaluator()\n",
    "#silhouette = evaluator.evaluate(predictions)\n",
    "#print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# 输出预测结果\n",
    "print(\"predicted Center: \")\n",
    "for center in predictions[['prediction']].collect():\n",
    "    print(center.asDict())\n",
    "\n",
    "# 聚类中心\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.GMM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussians shown as a DataFrame: \n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|mean                                                         |cov                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[9.099999999999985,9.099999999999985,9.099999999999985]      |0.006666666666783764  0.006666666666783764  0.006666666666783764  \n",
      "0.006666666666783764  0.006666666666783764  0.006666666666783764  \n",
      "0.006666666666783764  0.006666666666783764  0.006666666666783764  |\n",
      "|[0.10000000000001552,0.10000000000001552,0.10000000000001552]|0.006666666666806455  0.006666666666806455  0.006666666666806455  \n",
      "0.006666666666806455  0.006666666666806455  0.006666666666806455  \n",
      "0.006666666666806455  0.006666666666806455  0.006666666666806455  |\n",
      "+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"GaussianMixtureExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(0)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "model.gaussiansDF.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.关联规则\n",
    "我这里是pyspark 2.2以下的版本的写法，新版可以参考此程序之下的程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=[u'z'], freq=5)\n",
      "FreqItemset(items=[u'x'], freq=4)\n",
      "FreqItemset(items=[u'x', u'z'], freq=3)\n",
      "FreqItemset(items=[u'y'], freq=3)\n",
      "FreqItemset(items=[u'y', u'x'], freq=3)\n",
      "FreqItemset(items=[u'y', u'x', u'z'], freq=3)\n",
      "FreqItemset(items=[u'y', u'z'], freq=3)\n",
      "FreqItemset(items=[u'r'], freq=3)\n",
      "FreqItemset(items=[u'r', u'x'], freq=2)\n",
      "FreqItemset(items=[u'r', u'z'], freq=2)\n",
      "FreqItemset(items=[u's'], freq=3)\n",
      "FreqItemset(items=[u's', u'y'], freq=2)\n",
      "FreqItemset(items=[u's', u'y', u'x'], freq=2)\n",
      "FreqItemset(items=[u's', u'y', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u's', u'y', u'z'], freq=2)\n",
      "FreqItemset(items=[u's', u'x'], freq=3)\n",
      "FreqItemset(items=[u's', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u's', u'z'], freq=2)\n",
      "FreqItemset(items=[u't'], freq=3)\n",
      "FreqItemset(items=[u't', u'y'], freq=3)\n",
      "FreqItemset(items=[u't', u'y', u'x'], freq=3)\n",
      "FreqItemset(items=[u't', u'y', u'x', u'z'], freq=3)\n",
      "FreqItemset(items=[u't', u'y', u'z'], freq=3)\n",
      "FreqItemset(items=[u't', u's'], freq=2)\n",
      "FreqItemset(items=[u't', u's', u'y'], freq=2)\n",
      "FreqItemset(items=[u't', u's', u'y', u'x'], freq=2)\n",
      "FreqItemset(items=[u't', u's', u'y', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u't', u's', u'y', u'z'], freq=2)\n",
      "FreqItemset(items=[u't', u's', u'x'], freq=2)\n",
      "FreqItemset(items=[u't', u's', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u't', u's', u'z'], freq=2)\n",
      "FreqItemset(items=[u't', u'x'], freq=3)\n",
      "FreqItemset(items=[u't', u'x', u'z'], freq=3)\n",
      "FreqItemset(items=[u't', u'z'], freq=3)\n",
      "FreqItemset(items=[u'p'], freq=2)\n",
      "FreqItemset(items=[u'p', u'r'], freq=2)\n",
      "FreqItemset(items=[u'p', u'r', u'z'], freq=2)\n",
      "FreqItemset(items=[u'p', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q'], freq=2)\n",
      "FreqItemset(items=[u'q', u'y'], freq=2)\n",
      "FreqItemset(items=[u'q', u'y', u'x'], freq=2)\n",
      "FreqItemset(items=[u'q', u'y', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q', u'y', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q', u't'], freq=2)\n",
      "FreqItemset(items=[u'q', u't', u'y'], freq=2)\n",
      "FreqItemset(items=[u'q', u't', u'y', u'x'], freq=2)\n",
      "FreqItemset(items=[u'q', u't', u'y', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q', u't', u'y', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q', u't', u'x'], freq=2)\n",
      "FreqItemset(items=[u'q', u't', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q', u't', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q', u'x'], freq=2)\n",
      "FreqItemset(items=[u'q', u'x', u'z'], freq=2)\n",
      "FreqItemset(items=[u'q', u'z'], freq=2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"FPGrowthExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = spark.sparkContext.textFile(\"data/mllib/sample_fpgrowth.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"FPGrowthExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], [\"id\", \"items\"])\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
    "model = fpGrowth.fit(df)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model.transform(df).show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.LDA主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1:1 2:2 3:6 4:0 5:2 6:3 7:1 8:1 9:0 10:0 11:3\r\n",
      "1 1:1 2:3 3:0 4:1 5:3 6:0 7:0 8:2 9:0 10:0 11:1\r\n",
      "2 1:1 2:4 3:1 4:0 5:0 6:4 7:9 8:0 9:1 10:2 11:0\r\n",
      "3 1:2 2:1 3:0 4:3 5:0 6:0 7:5 8:0 9:2 10:3 11:9\r\n",
      "4 1:3 2:1 3:1 4:9 5:3 6:0 7:2 8:0 9:0 10:1 11:3\r\n"
     ]
    }
   ],
   "source": [
    "! head -5 data/mllib/sample_lda_libsvm_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -806.81672765\n",
      "The upper bound on perplexity: 3.10314127288\n",
      "\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[4, 7, 10] |[0.10782283322528141, 0.09748059064869798, 0.09623489511403283]|\n",
      "|1    |[1, 6, 9]  |[0.16755677717574005, 0.14746677066462868, 0.12291625834665457]|\n",
      "|2    |[1, 3, 9]  |[0.10064404373379261, 0.10044232016910744, 0.09911430786912553]|\n",
      "|3    |[3, 10, 4] |[0.2405485337093881, 0.11474862445349779, 0.09436360804237896] |\n",
      "|4    |[9, 10, 3] |[0.10479881323144603, 0.10207366164963672, 0.0981847998287497] |\n",
      "|5    |[8, 5, 7]  |[0.10843492932441408, 0.09701504850837554, 0.09334497740169005]|\n",
      "|6    |[8, 5, 0]  |[0.09874156843227488, 0.09654281376143092, 0.09565958598645523]|\n",
      "|7    |[9, 4, 7]  |[0.11252485087182341, 0.09755086126590837, 0.09643430677076377]|\n",
      "|8    |[4, 1, 2]  |[0.10994282164614115, 0.09410686880245682, 0.09374715192052394]|\n",
      "|9    |[5, 4, 0]  |[0.1526594065996145, 0.1401540984288492, 0.13878637240223393]  |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "transform dataset:\n",
      "\n",
      "+-----+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                       |\n",
      "+-----+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.004830688530254547,0.9563372032839312,0.004830653288196159,0.0049247000529390305,0.0048306686997597464,0.004830691229644231,0.004830725952841193,0.0048306754566327355,0.004830728026376915,0.004923265479424051]    |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.008057778649104173,0.3148301429775185,0.0080578223830065,0.008215777942952482,0.008057720361154553,0.008057732489412228,0.008057717726124932,0.00805779103670622,0.008057840506543925,0.6205496759274765]            |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.00419974114073206,0.9620399748900924,0.004199830998962131,0.0042814231963878655,0.004199801535688566,0.004199819689459903,0.004199830433436027,0.0041997822111186295,0.004199798534630995,0.0042799973694913]        |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.0037148958393689426,0.5313564622081751,0.00371492700514763,0.4388535874884561,0.0037150382511682853,0.0037149506801198505,0.0037149808253623792,0.0037148901801274804,0.0037149076678115434,0.003785359854262734]    |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.0040247360335797875,0.004348642552867576,0.004024775025300721,0.9633765038034603,0.004024773228145383,0.004024740478088116,0.00402477627651187,0.004024779618260475,0.004024784270292531,0.004101488713493013]       |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.003714916663186164,0.004014116840889892,0.0037150323955768686,0.003787652360887051,0.0037149873236278505,0.003714958841217428,0.0037149705182189397,0.003715010255807931,0.0037149614099447853,0.9661933933906431]   |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.003863635977009055,0.46449322935025966,0.0038636657354113126,0.5045241029221541,0.00386374420636613,0.0038636976398721237,0.003863727255143564,0.0038636207140121358,0.003863650494529744,0.003936925705242072]      |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004390966123798511,0.004744425233669778,0.004391025010757086,0.9600440191238313,0.004391023986304413,0.00439098335688734,0.004391015731875719,0.004391018535344605,0.0043910130377361935,0.004474509859794904]       |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004391082402111978,0.0047448016288253025,0.004391206864616806,0.004477234571510909,0.004391077028823487,0.004391110359190354,0.004391102894332411,0.004391148031605367,0.004391148275359693,0.9600400879436237]      |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.0033302167331450425,0.9698997342829896,0.003330238365882342,0.003394964707825143,0.0033302157712121493,0.0033302303649837654,0.0033302236683277224,0.0033302294595984666,0.0033302405714942906,0.0033937060745413443]|\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004199896541927494,0.00453848296824474,0.004200002237282065,0.9617819044818944,0.004200011124996577,0.004199942048495426,0.004199991764268097,0.004200001048497312,0.004199935367663148,0.004279832416731015]        |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.004830560338779577,0.005219247495550288,0.004830593014957423,0.004924448157616727,0.00483055816775155,0.004830577856153918,0.004830584648561171,0.00483060040145597,0.004830612377397914,0.9560422175417754]         |\n",
      "+-----+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"LDAExample\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 加载数据\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_lda_libsvm_data.txt\")\n",
    "\n",
    "# 训练LDA模型\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp)+\"\\n\")\n",
    "\n",
    "# 输出主题\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# 数据集解析\n",
    "print(\"transform dataset:\\n\")\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PCAExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 构建一份fake data\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# PCA降维\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### word2vec词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      word|              vector|\n",
      "+----------+--------------------+\n",
      "|     heard|[0.08829052001237...|\n",
      "|       are|[-0.1314301639795...|\n",
      "|      neat|[0.09875790774822...|\n",
      "|   classes|[-0.0047773420810...|\n",
      "|         I|[0.15081347525119...|\n",
      "|regression|[-0.0732467696070...|\n",
      "|  Logistic|[0.04169865325093...|\n",
      "|     Spark|[-0.0096837198361...|\n",
      "|     could|[-0.0907106027007...|\n",
      "|       use|[-0.1245830804109...|\n",
      "|        Hi|[0.03222155943512...|\n",
      "|    models|[0.15642452239990...|\n",
      "|      case|[-0.1072710305452...|\n",
      "|     about|[0.13248910009860...|\n",
      "|      Java|[0.08521263301372...|\n",
      "|      wish|[0.02581630274653...|\n",
      "+----------+--------------------+\n",
      "\n",
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.0788261869922,-0.00265940129757,0.0531761907041]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.00935709210379,-0.015802019309,0.0161747672329]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.0184408299625,-0.012609430775,0.0135096866637]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Word2VecExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# 输入是bag of words形式\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# 设置窗口长度等参数，词嵌入学习\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "# 输出词和词向量\n",
    "model.getVectors().show()\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
