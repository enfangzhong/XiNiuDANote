{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 简单爬虫与数据抓取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/web_crawler.jpg)\n",
    "网络爬虫一个非常炫酷，同时又非常实用的技术，这个部分我们给大家介绍如何写程序去完成网络数据的抓取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests是一个很实用的Python HTTP客户端库，编写爬虫和测试服务器响应数据时经常会用到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/requests.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要安装requests，最方便快捷的方法是使用pip进行安装。在命令行执行\n",
    "```shell\n",
    "pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.请求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的静态网页数据获取，需要先对网页进行请求，比如我们以腾讯新闻为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = requests.get('http://news.qq.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 状态码\n",
    "我们可以从返回result的状态码中，了解到是否请求成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "200表示正常返回结果，其余的数字(比如404，500等)表示请求未拿到正常返回结果。咱们来试几个网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status(url):\n",
    "    r = requests.get(url, allow_redirects=False)\n",
    "    return r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(get_status('http://www.zhidaow.com'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "print(get_status('http://www.zhidaow.com/hi404/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "print(get_status('http://www.baidu.com/link?url=QeTRFOS7TuUQRppa0wlTJJr6FfIYI1DJprJukx4Qy0XnsDO_s9baoO8u1wvjxgqN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 文件编码\n",
    "我们的最终目的是获取显示在网页上的内容，文本字符串有不同的编码格式，咱们可以先从返回结果里做一个简单的了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GB2312'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到网页的编码格式为GB2312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 返回的网页内容\n",
    "从返回的结果result中，可以取出网页的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"renderer\" content=\"webkit\" />\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\\n    <title>\\u65b0\\u95fb\\u4e2d\\u5fc3_\\u817e\\u8baf\\u7f51</title>\\n    <meta name=\"keywords\" content=\"\\u65b0\\u95fb \\u65b0\\u95fb\\u4e2d\\u5fc3 \\u4e8b\\u5b9e\\u6d3e \\u65b0\\u95fb\\u9891\\u9053,\\u65f6\\u4e8b\\u62a5\\u9053\">\\n    <meta name=\"description\" content=\"\\u817e\\u8baf\\u65b0\\u95fb\\uff0c\\u4e8b\\u5b9e\\u6d3e\\u3002\\u65b0\\u95fb\\u4e2d\\u5fc3,\\u5305\\u542b\\u6709\\u65f6\\u653f\\u65b0\\u95fb\\u3001\\u56fd\\u5185\\u65b0\\u95fb\\u3001\\u56fd\\u9645\\u65b0\\u95fb\\u3001\\u793e\\u4f1a\\u65b0\\u95fb\\u3001\\u65f6\\u4e8b\\u8bc4\\u8bba\\u3001\\u65b0\\u95fb\\u56fe\\u7247\\u3001\\u65b0\\u95fb\\u4e13\\u9898\\u3001\\u65b0\\u95fb\\u8bba\\u575b\\u3001\\u519b\\u4e8b\\u3001\\u5386\\u53f2\\u3001\\u7684\\u4e13\\u4e1a\\u65f6\\u4e8b\\u62a5\\u9053\\u95e8\\u6237\\u7f51\\u7ad9\">\\n    <meta name=\"author\" content=\"skeetershi\" />\\n  \\t<meta name=\"applicable-device\" content=\"pc,mobile\">\\n\\t<link rel=\"alternate\" media=\"only screen and (max-width: 640px)\" href=\"https://xw.qq.com/\">\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"//mat1.gtimg.com/news/skeetershi/news_index/css/index.min_071107.css\">\\n    <!-- 2016.7.13 jackiejiang add house.com use-->\\r\\n<script src=\"//mat1.gtimg.com/house/js/h5rewrite.js\"></script>\\r\\n\\r\\n<!--2014.3.20 byAustinjin-->\\r\\n<!--20140225 menshen-->\\r\\n<script type=\"text/javascript\" src=\"//js.aq.qq.com/js/aq_common.js\"></script><!--[if !IE]>|xGv00|135a509a6ac85759a2a10161f645f1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上为网页的HTML格式数据，接着我们就可以从上面取出所需要的内容了，比如现在我们需要爬取的是这一页腾讯新闻的新闻标题。<br>\n",
    "解析的方式有非常多种，比如有一个叫做BeautifulSoup的库，我们可以用它完成网页返回结果的解析和内容的提取。感兴趣的同学可以参考[它的中文文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)\n",
    "\n",
    "我们这里讲一种比较高效的套路，叫做xpath路径表达式，具体的方法如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector = etree.HTML(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先试试解析新闻的标题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = selector.xpath('//div[@class=\"text\"]/em/a/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去库存政策持续发力 80城住宅库存同比连降27个月\n",
      "央行官员：要打破垄断 允许更多主体参与金融市场\n",
      "波黑将成第二个对中国实行全面免签欧洲国家\n",
      "港媒：中国推中泰高铁 拟年底前启动第一阶段工程\n",
      "李明哲犯颠覆国家政权罪 被判处有期徒刑5年\n",
      "教育新规：节假日、寒暑假老师不能组织学生补课、自习、考试\n",
      "广州全面实施禁酒控酒 一般公务活动全面禁酒\n",
      "广东潮州原市长卢淳杰深圳受审 被控受贿千万\n",
      "最假的假新闻！“李嘉诚跑了”的谣言何以频频上演\n",
      "四川“悬崖村”天梯将安装微光路灯\n",
      "优等少年杀人事件：74刀！跑进黑暗离奇的14年\n",
      "发布突发事件信息，政府要学会“退居幕后”\n",
      "宁波警方通报爆炸案：拆解爆炸物不慎引发 排除人为故意\n",
      "中国将在北京等9省市区扩大水资源税改革试点\n",
      "醉驾男子累计社会服务30小时 拿到检方不起诉决定书\n",
      "她在火车站狂扇工作人员耳光 网友：谁给的胆子?\n",
      "分手后再纠缠 女子过失致前男友死亡\n",
      "台湾“独派”拆蒋介石像 被讽：在台珍宝也应还给大陆\n",
      "056A型护卫舰遂宁舰28日正式加入人民海军战斗序列\n",
      "男子赴外地谈生意不成 偷走价值9万元铜渣\n",
      "拖欠工资拒不支付 藏匿外地难逃法网\n",
      "山西临沂蜜桃打防腐剂 虚假网络视频案宣判\n",
      "�望：全面深改 要坚守“三条底线”不动摇\n",
      "不管多小的官只要伸了手 都难逃被捉的宿命\n",
      "香港立法会向4名前辱国议员追讨薪水 共计1174万港元\n",
      "2017中科院院士增选结果揭晓 包括2名诺贝尔奖获得者\n",
      "港媒：中泰高铁第一阶段工程拟于年底前启动\n",
      "宁波警方通报爆炸案进展：爆炸物归属者系逃犯 已被抓获\n",
      "彭宇华、李明哲犯颠覆国家政权罪分别获刑7年和5年\n",
      "私家车返修途中自燃烧毁 汽修中心及无资质承租个人被判连带赔偿\n",
      "全国首家以律师为调解员征地拆迁纠纷调解中心成立\n",
      "“银行柜台机前杀人案”被告人一审被判死刑\n",
      "巴厘岛机场延长关闭24小时 外媒称近6万名旅客滞留\n",
      "楼市深度调控下：50宗“地王”为何仅有一成入市？\n",
      "“限韩”破冰：中国为什么必须进口BIGBANG和EXO\n",
      "茅台结束七连跌大涨超4% 市值重回8000亿元\n",
      "汕头大学回应李嘉诚基金会办公室被摘牌：传闻不实\n",
      "被爆料为比特币创始人 马斯克本人如此回应\n",
      "贾跃亭雪上加霜！FF或无法完成5亿美元融资\n",
      "监管窗口指导公募基金大单买卖 业内称属“日常操作”不意外\n",
      "北京二手房成交量同比减半 中介离职潮再现\n",
      "中企回应瓜达尔港收益分成质疑：已充分考虑巴方利益\n",
      "69家央企集团公司制改制进入尾声 涉及8万亿总部资产\n",
      "【一线】洲际酒店集团大中华区CEO首秀：开放更多特许经营\n",
      "对话希拉里：经济好要谢奥巴马，特朗普忽视了几件大事\n",
      "投资中国崛起计划：赌国运成功的人 从未曾被亏待过\n",
      "21省试水聘任制公务员 有猎头公司出3倍工资挖人\n",
      "秦朔：我们离人越近，灰犀牛就离我们越远\n",
      "港媒：中国续推中泰高铁项目 拟年底前启动第一阶段工程\n",
      "马云：没有一个欧美国家敢想象超越中国的快递业\n",
      "消费品进口关税3年4降 谁还会去日本抢马桶盖\n",
      "80城住宅库存同比连降27个月 规模回落到4年前\n",
      "互金协会否认现金贷“一刀切” 成员需报备现金贷业务\n",
      "闻泰科技乱局：大股东要清仓减持，200亿市值韭菜抗？\n",
      "特朗普税改迎来最关键一周 美股盯紧参院投票\n",
      "“辱母案”后：于欢母亲涉集资诈骗被刑拘 涉案18起\n",
      "2017年A股上市公司市值报告：7成缩水，53家公司翻倍\n",
      "沙河淘宝店“跑路”真相调查：“三低”商业模式转型前夜的阵痛\n",
      "宜家北美召回1730万个问题抽屉柜 上海仍有售\n",
      "厦华电子拟“卖子”还未回复问询 交易对手突毁约\n",
      "国家能源集团正式成立 资产规模超1.8万亿元\n",
      "雷军当选全国工商联副主席：压力巨大，一定认真履职！\n",
      "逾20省份试水聘任制公务员 打破铁饭碗如何实现常态化？\n",
      "90后每个月收入多少才正常？你拖后腿了吗\n",
      "钮文新：特朗普不会放弃美元霸权\n",
      "外交部回应中国解禁部分赴韩旅游团：不了解情况\n",
      "京东金融和工行合作的165天\n",
      "鲍威尔力挺美联储继续加息 黄金涨势受阻\n",
      "为什么现金贷36%利率红线是合理的？\n",
      "中国时速600公里高铁或将诞生 京沪直达只需2小时\n",
      "创始人跑路 用户押金难退：共享单车为何一地鸡毛？\n",
      "最可能被淘汰职业排行：从事这12个职业的人危险了\n",
      "买房置业进入“她”时代 城市女主人豪过男同胞\n",
      "房企内地融资渠道收窄 海外融资难度也在加大\n",
      "日企造假名单又增一家 外媒称日本制造出现系统性危机\n",
      "月薪10万港元才算中产 香港中产阶层面临分化危机\n",
      "辩证看待民间投资增速持续放缓\n",
      "国际油价上扬 周四国内成品油调价或搁浅\n",
      "张朝阳：我们真的没有触碰电子商务\n",
      "上海三互金平台回应停贷风波 乌龙了？\n",
      "你买的房子到底值多少钱？取决于这五大因素\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detail_urls = selector.xpath('//div[@class=\"text\"]/em/a/@href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.qq.com/a/20171128/000651.htm'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-11-28 04:04']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = requests.get(detail_urls[0])\n",
    "news_selector = etree.HTML(news.text)\n",
    "time = news_selector.xpath('//span[@class=\"a_time\"]/text()')\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 在URLs中传递参数\n",
    "有时候我们需要在URL中传递参数，比如在采集百度搜索结果时，我们wd参数（搜索词）和rn参数（搜素结果数量），你可以手工组成URL，requests也提供了一种看起来很NB的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/s?rn=10&wd=%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%96%87%E6%91%98\n"
     ]
    }
   ],
   "source": [
    "my_params = {'wd':'大数据文摘', 'rn':'10'}\n",
    "r = requests.get(\"http://www.baidu.com/s\", params=my_params)\n",
    "print r.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 设置超时时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过timeout属性设置超时时间，一旦超过这个时间还没获得响应内容，就会提示错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPConnectionPool(host='github.com', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f80aaed3790>, 'Connection to github.com timed out. (connect timeout=0.001)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mConnectTimeout\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2754616ff2cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://github.com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    511\u001b[0m         }\n\u001b[1;32m    512\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0;31m# TODO: Remove this in 3.0.0: see #2811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNewConnectionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPConnectionPool(host='github.com', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f80aaed3790>, 'Connection to github.com timed out. (connect timeout=0.001)'))"
     ]
    }
   ],
   "source": [
    "requests.get('http://github.com', timeout=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 添加代理\n",
    "爬虫爬取数据时为避免被封IP，经常会使用代理。requests也有相应的proxies属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "proxies = {\n",
    "  \"http\": \"http://10.10.1.10:3128\",\n",
    "  \"https\": \"http://10.10.1.10:1080\",\n",
    "}\n",
    "requests.get(\"http://www.zhidaow.com\", proxies=proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果代理需要账户和密码，则需这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proxies = {\n",
    "    \"http\": \"http://user:pass@10.10.1.10:3128/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 请求头内容\n",
    "请求头内容可以用r.request.headers来获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Connection': 'keep-alive', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'User-Agent': 'python-requests/2.17.3'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 模拟登陆\n",
    "有很多网站需要我们登录之后才能完成数据的抓取，requests同样支持提交表单(包含用户名和密码)进行登录。代码的示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
